from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch


# åŠ è½½æ•°æ®é›†
dataset = load_dataset("cnn_dailymail", "3.0.0")
train_data = dataset["train"].select(range(10000))
val_data = dataset["validation"].select(range(10000))

print("ğŸŒ°Train data sample", train_data[0])


# æ ¼å¼åŒ–æ•°æ®é›†
def format_sample(example):
    return {
        "input": f"[INST] Summarize the following article:\n{example['article']} [/INST]",
        "output": example["highlights"]
    }

train_data = train_data.map(format_sample)
val_data = val_data.map(format_sample)


# åŠ è½½ tokenizer å’Œæ¨¡å‹
model_name = "microsoft/phi-3-mini-4k-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",           # è‡ªåŠ¨æ”¾å…¥GPU
    torch_dtype="auto",          # ä»¥æœ€ä½æ˜¾å­˜éœ€æ±‚åŠ è½½
    trust_remote_code=True
)
model.eval()



def get_teacher_outputs(input_text, tokenizer, model):
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=1024).to(model.device)
    
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True, output_attentions=True,return_dict=True)
    
    return {
        "logits": outputs.logits,
        "hidden_states": outputs.hidden_states
    }

# # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œä½¿ç”¨äº†BERTæ¨¡å‹ï¼Œä»»åŠ¡æ˜¯äºŒåˆ†ç±»
# model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
# tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

# # åŠ è½½IMDbçš„æ•°æ®é›†
# dataset = load_dataset("cnn_dailymail", "3.0.0")
# train_data = dataset["train"].shuffle(seed = 47)
# val_data = dataset["validation"]


# # å®šä¹‰åˆ†è¯å‡½æ•°
# def tokenize(batch):
#     return tokenizer(batch["text"], padding=True, truncation=True)

# # æ‰¹é‡å¤„ç†æ•´ä¸ªæ•°æ®é›†å¹¶è®¾ç½®æ ¼å¼ä¸ºPyTorch Tensor
# dataset = dataset.map(tokenize, batched=True)
# dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])

# # å®šä¹‰è®­ç»ƒå‚æ•°
# training_args = TrainingArguments(
#     output_dir="./checkpoints/bert", # è¾“å‡ºç›®å½•
#     evaluation_strategy="epoch", # æ¯è½®è¯„ä¼°
#     save_strategy="epoch", # æ¯è½®ä¿å­˜
#     per_device_train_batch_size=8, # æ¯å¼  GPU æ¯ä¸ª step è®­ç»ƒçš„æ ·æœ¬æ•°
#     per_device_eval_batch_size=8, # æ¯å¼  GPU æ¯ä¸ª step éªŒè¯çš„æ ·æœ¬æ•°
#     num_train_epochs=2, # è®­ç»ƒè½®æ•°
#     logging_dir="./logs", # æ—¥å¿—ç›®å½•
#     logging_steps=500, # è®­ç»ƒ500æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
#     save_total_limit=2, # checkpointæ•°é‡ä¸Šé™
#     fp16=True if torch.cuda.is_available() else False #å¦‚æœæœ‰ GPUï¼Œå°±ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16ï¼‰
# )

# # å®šäºè®­ç»ƒå™¨
# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=dataset["train"].shuffle(seed=42), # æ‰“ä¹±é¡ºåº
#     eval_dataset=dataset["test"] # ä¸åº”è¯¥æ‰“ä¹±é¡ºåº
# )
# # å¯åŠ¨è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹
# trainer.train()
# model.save_pretrained("./models/bert")
# tokenizer.save_pretrained("./models/bert")