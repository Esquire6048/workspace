https://medium.com/%40rizqimulkisrc/edge-llms-running-large-models-on-resource-constrained-devices-e27f6e39f3ef



https://arxiv.org/abs/2403.11401?utm_source=chatgpt.com



https://arxiv.org/abs/2409.14457v2



CoSLAM: Collaborative Visual SLAM for Multiple Cameras/Robots
早期多视角 SLAM 框架，通过跨设备回环检测和地图融合提升重定位与建图鲁棒性（例如 Ma 等人，2013）。



ORB-SLAM3 Multi-Agent Extension
在 ORB-SLAM3 基础上，多个机器人共享子图与关键帧，实现大规模室内空间的协同建图与定位。



SemanticFusion & Kimera
将语义分割结果融入 SLAM，多个移动设备可共享语义子图，提升多房间环境下的语义理解与路径规划。



Decentralized SLAM: Multi-Robot Cooperative Mapping (2022)
在无中心节点情况下，多机器人交换子图与回环检测信息



MAVNet: Multi-Agent Visual Navigation (2023)
基于 GNN 的协同路径规划与场景认知，支持局部信息共享



C-ORB-SLAM: Cloud-Assisted ORB-SLAM for Indoor (2024)
将部分计算卸载至私有云，在局域网内实现多机地图共享



SemanticMapFusion (2023)
多摄像头/多机器人共享语义分割子图，融合后可实时构建带语义的室内地图



Collaborative Visual-Language Navigation (2025)
结合 Vision-Language 模型与多机器人系统，实现基于自然语言指令的室内协同导航



Point2Graph: An End-to-end Point Cloud-based 3D Open-Vocabulary Scene Graph for Robot Navigation
这篇论文提出了一个名为 **Point2Graph** 的端到端点云（Point Cloud）驱动的 3D 开放词汇（Open-Vocabulary）场景图生成框架，专为机器人导航设计。下面从背景与动机、总体架构、关键模块、实验与结果，以及贡献与意义五个方面，用中文详细展开说明。

---

## 一、背景与动机

在室内机器人导航与路径规划中，**3D 场景图（3D Scene Graph）** 能以结构化的方式表示房间（room）、物体（object）及它们之间的空间层级和语义关系，极大提升机器人对环境的理解与交互能力。
然而，现有的**开放词汇场景图生成**算法（如 HOV-SG、ConceptGraphs）通常依赖于对齐好的 RGB-D 图像序列及其相机位姿来将视觉特征投影到 3D 点云，并通过特征蒸馏或跨模态检索来实现开放词汇分类。这在真实场景下往往难以满足——要么缺少 RGB-D 图像，要么无法准确获取相机位姿，甚至常因遮挡、视角受限导致投影不完整。

**Point2Graph** 的核心动机在于：

> \*\*消除对 RGB-D 图像及其位姿的依赖，\*\*仅以 3D 点云为输入，**端到端**地产生房间与物体层面的开放词汇场景图。

---

## 二、总体框架

整体上，Point2Graph 包含两层层级：

1. **房间层（Room Level）**
2. **物体层（Object Level）**

输入：完整场景的 3D 点云
输出：节点为房间和物体、边表示“包含”关系的 3D 场景图

```
3D 点云
  ├─ 房间分割 + 分类  → Room Nodes
  └─ 物体检测 + 分类  → Object Nodes
合并房间−物体层级信息 → 生成最终场景图
```

---

## 三、关键模块

### 1. 房间分割与开放词汇分类

* **几何边界检测 + Transformer 区域检测融合**

  * **几何边界检测**：基于点云密度与法线信息，在房间之间的墙壁边缘生成边界候选。
  * **学习式区域检测**：使用 Transformer 网络对点云区域进行聚类分割，捕捉语义一致的房间区域。
  * 两者融合提高了在复杂室内环境中分割各个房间的鲁棒性与准确度。
* **“Snap-Lookup” 开放词汇分类**

  * 对于每个分割出的房间子点云，随机采样多个“快照”（snapshots），并在一个**文本-图像检索**框架中将其特征与大规模视觉-语言模型（如 CLIP）的文本词库进行相似度匹配，直接输出开放词汇房间类型（例如：厨房、会议室、走廊等），无需任何房间专用的有监督标注数据。

### 2. 物体检测与开放词汇分类

* **Class-agnostic 3D 检测器 + DBSCAN 精炼**

  * 先对整个场景点云应用**无类别（class-agnostic）3D 目标检测**，生成一系列候选包围盒。
  * 利用 **DBSCAN** 对包围盒内的点云进行聚类过滤，去除噪声和过小的误检。
* **跨模态检索分类**

  * 对于每个经过精炼的候选物体子点云，同样采用“文本检索（text query）”机制，将其嵌入与文本描述空间对齐，输出开放词汇物体类别（如：椅子、桌子、垃圾桶等），同样无需依赖 RGB-D 图像或额外标注。

---

## 四、实验与结果

* **数据集与基准**：在多个真实室内场景点云数据集（室内扫描、工业大楼模型等）上，对比了当前 SOTA 的开放词汇房间/物体分割与分类方法。
* **性能表现**：

  * 在房间分割精度、房间分类准确率上均优于 HOV-SG、ConceptGraphs 等方法；
  * 在物体检测与分类上，也实现了更高的 mAP（mean Average Precision）和更低的误检率。
* **机器人导航验证**：将生成的场景图应用于实际助老助残移动机器人导航，验证了基于 Point2Graph 场景图的路径规划与语义查询（如“去最近的卫生间”）的可行性和效率提升。

---

## 五、贡献与意义

1. **端到端点云驱动**：首次提出完全不依赖 RGB-D 图像和相机位姿，仅以 3D 点云构建开放词汇 3D 场景图的框架。
2. **混合分割策略**：几何与学习方法融合的房间分割机制，显著提高了对真实复杂室内环境的适应性。
3. **Snap-Lookup 检索**：通用的文本检索分类模块，可无缝扩展到任意开放词汇类别，便于机器人在新环境中理解未知空间和物体。
4. **高效导航验证**：在助行机器人上完成实地导航实验，展现了框架在真实应用中的潜力。

> **总结**：Point2Graph 将 3D 点云的纯几何信息与大规模视觉-语言模型的开放词汇能力结合，为室内机器人实现更通用、更鲁棒的场景理解与导航奠定了新基础。未来可进一步探索其在室外点云、动态场景以及更细粒度关系（如物体–物体交互）建模方面的扩展。



Learning Distilled Collaboration Graph for Multi-Agent Perception
以下内容基于公开的论文和补充资料，分模块详述《Learning Distilled Collaboration Graph for Multi-Agent Perception》的核心思想与实验验证。

**1. 研究背景与动机**
传统的单智能体感知（single-agent perception）难以应对遮挡和远距目标识别等挑战。多智能体协同感知通过共享信息能显著提升整体感知能力，但也带来了通信带宽开销过大的问题([arxiv.org][1])。常见的三种协同策略：

* **早期融合（Early Collaboration）**：直接共享原始测量数据（如 LiDAR point cloud），对整体场景有最完整的视野，但通信量极大；
* **后期融合（Late Collaboration）**：仅共享各自的检测结果（bounding boxes 等），带宽开销小，但融合信息片面、噪声多；
* **中期融合（Intermediate Collaboration）**：共享中间特征（feature map），在性能与带宽之间权衡，但关键在于如何设计高效且自适应的特征融合策略([arxiv.org][1])。

**2. 核心方法——蒸馏协作图（DiscoGraph）**
作者提出了**蒸馏协作图**（DiscoGraph）来建模多智能体间的*可训练、姿态感知（pose-aware）且自适应*的协作关系，方法包括两大创新：

1. **教师–学生框架的知识蒸馏**

   * \*\*教师模型（Teacher）\*\*基于早期融合，输入为所有智能体的*全景（holistic-view）数据*，获得最强的联合特征表达；
   * \*\*学生模型（Student）\*\*基于中期融合，仅接收各自单视角（single-view）特征；
   * 通过约束学生模型后融合的特征映射，使其在空间对应关系上与教师模型保持一致，从而“蒸馏”出高质量的协作策略([arxiv.org][1])。

2. **矩阵化边权（Matrix-valued Edge Weight）**

   * 在传统图模型中，边权通常为标量；DiscoGraph 将每条边定义为一个矩阵，其中矩阵的每个元素表示两智能体在某一空间区域上的注意力（attention）强度；
   * 该设计使得图模型不仅能学习“哪个智能体与哪个智能体协作”，还能自适应地突出“在哪些空间区域交换信息”([arxiv.org][1])。

**3. 数据集构建：V2X-Sim 1.0**
为验证方法，作者基于 CARLA 与 SUMO 联合仿真平台自建了**V2X-Sim 1.0**多智能体 3D 目标检测数据集：

* 在同一城市场景中随机生成 500 辆车，使其以随机路线行驶；
* 从日志中抽取 100 个场景，每个场景时长 20 秒，包含 2 至 5 个智能体（M=2,3,4,5）协同感知任务([proceedings.neurips.cc][2])。

**4. 实验设置与结果**

* **任务**：多智能体 BEV（Bird’s-Eye View）3D 目标检测；
* **对比方法**：Early Fusion、Late Fusion、当下主流中期融合方法（如 When2com、V2VNet 等）；
* **评价指标**：检测平均精度（mAP）、通信开销（bandwidth）；
* **结论**：

  * 蒸馏后的学生模型（DiscoNet）在带宽消耗大幅降低的同时，仍能逼近教师模型的检测性能；
  * 相较于其他中期融合方法，DiscoNet 在相同带宽预算下取得了更高的 mAP；
  * 并且所学图结构直观、可解释——能够聚焦于对检测最有价值的空间区域和智能体伙伴([arxiv.org][1])。

**5. 结论与展望**
该工作通过知识蒸馏巧妙地将“全局视角”带来的性能优势，转移到仅依赖单视角和稀疏通信的学生模型上，实现了多智能体协同感知在性能—带宽权衡上的重大突破。未来可进一步探索：

* 在真实 V2X 环境下的部署与验证；
* 将该蒸馏策略扩展到其他多模态（camera、radar）协同；
* 动态自适应地调整蒸馏目标与通信频率，进一步提升系统鲁棒性与效率。

[1]: https://arxiv.org/abs/2111.00643 "[2111.00643] Learning Distilled Collaboration Graph for Multi-Agent Perception"
[2]: https://proceedings.neurips.cc/paper/2021/file/f702defbc67edb455949f46babab0c18-Supplemental.pdf?utm_source=chatgpt.com "[PDF] Learning Distilled Collaboration Graph for Multi-Agent Perception"



